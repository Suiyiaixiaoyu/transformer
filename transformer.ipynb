{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](2019040813401989.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "import copy\n",
    "import numpy as np\n",
    "from torch import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,encoder,decoder,input_emd,out_emd,generate):\n",
    "        super(EncoderDecoder,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_emd = input_emd\n",
    "        self.out_emd = out_emd\n",
    "        self.generate = generate\n",
    "    \n",
    "    def forward(self,inputword,outputword,):\n",
    "        pass\n",
    "#整体框架，generate指的是decoder后的分类器       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate的搭建，linear和softmax\n",
    "class generate(nn.Module):\n",
    "    def __init__(self,out_size,vocabsize):\n",
    "        super(generate,self).__init__()\n",
    "        self.linear = nn.Linear(out_size,vocabsize)\n",
    "    def forward(self,x):\n",
    "        return F.log_softmax(self.linear(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "#encoder 和 decoder都是N层的，为了代码的精简这里用到了深层拷贝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder搭建\n",
    "class encoder(nn.Module):\n",
    "    def __init__(self,encoderlayer):\n",
    "        super(encoder,self).__init__()\n",
    "        self.layers = clones(encoderlayer,6)\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#残差网络的搭建，就是图中add+norml这块\n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self,sentance_size,word_size):\n",
    "        super(SublayerConnection,self).__init__()\n",
    "        self.layernorm = nn.LayerNorm([sentance_size,word_size],eps=1e-5)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    def forward(self,x,sublayer):\n",
    "        out = self.layernorm(x + self.dropout(sublayer(x)))\n",
    "        return out\n",
    "#这里使用layernorm是因为每个layer长度是不同的，为了能batch，我们会选择padding补0，这时候再使用BatchNormal就不合适了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#搭建encoderlayer\n",
    "class encoderlayer(nn.Module):\n",
    "    def __init__(self,feedforward,self_attn,sentance_size,word_size):\n",
    "        super(encoderlayer,self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feedforward = feedforward\n",
    "        self.layers = clones(SublayerConnection(sentance_size,word_size),2)\n",
    "    def forward(x,mask):\n",
    "        x = self.layers[0](x,lambda x:self.self_attn(x,x,x,mask))#这里使用lambda是因为SublayerConnection中我们输入的是一个函数\n",
    "        x = self.layers[1](x,self.feedforward)\n",
    "        return x\n",
    "#这里mask的目的是去除掉padding在训练过程中的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder的搭建\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        \n",
    "    def forward(self, x, memory, second_mask, first_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, second_mask, first_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#搭建decoderlayer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, sentance_size,word_size, self_attn, out_attn, feed_forward):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn  = self_attn #第一层attenation\n",
    "        self.out_attn = out_attn  #第二层attenation\n",
    "        self.feed_forward = feed_forward\n",
    "        self.layers = clones(SublayerConnection(sentance_size,word_size),3)\n",
    "    def forward(x,memory, second_mask, first_mask):\n",
    "        m = memory #指定是enconder的输出\n",
    "        x = self.layers[0](x,lambda x :self.self.self_attn(x,x,x,first_mask))\n",
    "        x = self.layers[1](x,lambda x :self.self.out_attn(m,m,x,second_mask))\n",
    "        x = self.layers[2](x,self.feedforward)\n",
    "        return x\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False, False],\n",
      "        [ True,  True, False, False],\n",
      "        [ True,  True,  True, False],\n",
      "        [ True,  True,  True,  True]])\n",
      "tensor([[False, False, False, False],\n",
      "        [ True, False, False, False],\n",
      "        [ True,  True, False, False],\n",
      "        [ True,  True,  True, False]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m = np.ones((4,4))\n",
    "k1 = np.triu(m, k=1)\n",
    "k2 = np.triu(m, k=0)\n",
    "print(torch.from_numpy(k1) == 0)\n",
    "print(torch.from_numpy(k2) == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#制造mask 效果见上\n",
    "def subsequent_mask(word_size):\n",
    "    attn_shape = (1, word_size, word_size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](20190408141116387.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#搭建attenation\n",
    "def attention(query, key, value, mask=None):\n",
    "    dk = query.size(-1)\n",
    "    #用爱因斯坦求和约定torch.einsum('bij,bkj->bik')/torch.sqrt(dk),如果是多头的话有变化\n",
    "    s = torch.matmul(query,key.transpose(-2,-1))/torch.sqrt(dk) \n",
    "    if mask is not None:\n",
    "        s = s.masked_fill(mask == 0, -1e9)#把mask上为0的地方填充为负无穷\n",
    "    p = F.softmax(s,dim=-1)\n",
    "    return torch.matmul(p,value),p\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多头自注意力模型\n",
    "![title](transformer_38_0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self,word_size,h):\n",
    "        super(MultiHeadedAttention,self).__init__()\n",
    "        self.h = h\n",
    "        self.word_size = word_size\n",
    "        self.dk = word_size//h  #dk是每一个头的dim，这里用到了向下取整//，我比较喜欢头数能和wordsize整除\n",
    "        self.linears = clones(nn.Linear(word_size,word_size),4)\n",
    "        \n",
    "    def forward(self,query, key,value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        batches = query.size(0)\n",
    "        #这里是q,k,v分别经过linear层之后，拆成(b,h,s,dk)\n",
    "        query,key,value = [linear(x).view(batches,-1,self.h,self.dk).transpose(1,2) for linear,x in zip(self.linears,(query,key,value))]\n",
    "        x, _ = attention(query,key,value,mask)\n",
    "        x = x.transpose(1,2).contiguous().view(batches,-1,self.h*self.dk) #重新整合成（b，s, h*dk),让多头变成一个头\n",
    "        x = self.linears[-1](x)#经过最后一层linear\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feedforward简单的前馈神经网络，没啥好讲的\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, word_size, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(word_size, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, word_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding层也没啥好讲的\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, word_size, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.emd = nn.Embedding(vocab, word_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emd(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](20190918202641449.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, word_size, dropout, num_word=2000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        #word_size指的是词向量的维度，max_word是词向量的数量\n",
    "        pe = torch.zeros(num_word,word_size)\n",
    "        position = torch.arange(0,num_word).unsqueeze(0)\n",
    "        div = torch.pow(10000,torch.arange(0,num_word,2)/word_size)\n",
    "        pe[:,0::2] = torch.sin(position/div)\n",
    "        pe[:,1::2] = torch.cos(position/div)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)#把pe推入内存，并不参与梯度计算\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)    #word+word_position,根据你实际的词的多少，从pe中直接取得位置信息\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后面就不做了，具体实现，建议去看bert和gpt的源码，他们才是面试的时候会经常问的，bert采用的是transformer中encoder这一块，而GPT采用的是decoder这一块。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
